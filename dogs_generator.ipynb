{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "name": "logograms.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KhanhPham2411/how_to_generate_video/blob/master/dogs_generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FozVgeYJDIG",
        "colab_type": "text"
      },
      "source": [
        "# Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shC2o053JmqE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import AveragePooling2D\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import concatenate\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.utils import shuffle\n",
        "import pickle\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgWQ3lK6JuGV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "440f8751-95df-42e1-cc26-e6bdbaefdc86"
      },
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "from keras import backend as K\n",
        "\n",
        "# https://stackoverflow.com/questions/57797113/attributeerror-module-keras-backend-has-no-attribute-image-dim-ordering\n",
        "# image_dim_ordering change to image_data_format from Keras 2.x\n",
        "# K.set_image_dim_ordering('th') # ensure our dimension notation matches\n",
        "# https://github.com/keras-team/keras/issues/12649#issuecomment-615132968\n",
        "K.set_image_data_format('channels_first')\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.layers import Reshape\n",
        "from keras.layers.core import Activation\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.layers.convolutional import UpSampling2D\n",
        "from keras.layers.convolutional import Convolution2D, AveragePooling2D\n",
        "from keras.layers.core import Flatten\n",
        "from keras.optimizers import SGD, Adam\n",
        "from keras.datasets import mnist\n",
        "from keras import utils\n",
        "import numpy as np\n",
        "from PIL import Image, ImageOps\n",
        "import argparse\n",
        "import math\n",
        "\n",
        "import os\n",
        "import os.path\n",
        "\n",
        "import glob"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBl6w_22JyRj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "47261dda-41bd-49fd-d9cb-eb5849bcfcea"
      },
      "source": [
        "# https://github.com/keras-team/keras/issues/13684#issuecomment-595054461 - experimental_list_devices in tensorflow_backend.py\n",
        "import tensorflow as tf\n",
        "import keras.backend.tensorflow_backend as tfback\n",
        "\n",
        "print(\"tf.__version__ is\", tf.__version__)\n",
        "print(\"tf.keras.__version__ is:\", tf.keras.__version__)\n",
        "\n",
        "def _get_available_gpus():\n",
        "    \"\"\"Get a list of available gpu devices (formatted as strings).\n",
        "\n",
        "    # Returns\n",
        "        A list of available GPU devices.\n",
        "    \"\"\"\n",
        "    #global _LOCAL_DEVICES\n",
        "    if tfback._LOCAL_DEVICES is None:\n",
        "        devices = tf.config.list_logical_devices()\n",
        "        tfback._LOCAL_DEVICES = [x.name for x in devices]\n",
        "    return [x for x in tfback._LOCAL_DEVICES if 'device:gpu' in x.lower()]\n",
        "\n",
        "tfback._get_available_gpus = _get_available_gpus"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.__version__ is 2.2.0\n",
            "tf.keras.__version__ is: 2.3.0-tf\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upOA67-KJLPG",
        "colab_type": "text"
      },
      "source": [
        "#Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lwjn1cQ9JQTP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "c23c0242-255c-4ae4-9351-9f56617913d1"
      },
      "source": [
        "!wget --no-check-certificate \\\n",
        "  https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip \\\n",
        "  -O ./cats_and_dogs_filtered.zip"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-31 10:03:29--  https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 108.177.111.128, 2607:f8b0:4001:c14::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|108.177.111.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 68606236 (65M) [application/zip]\n",
            "Saving to: ‘./cats_and_dogs_filtered.zip’\n",
            "\n",
            "\r          ./cats_an   0%[                    ]       0  --.-KB/s               \r         ./cats_and  26%[====>               ]  17.06M  85.3MB/s               \r./cats_and_dogs_fil 100%[===================>]  65.43M   172MB/s    in 0.4s    \n",
            "\n",
            "2020-05-31 10:03:29 (172 MB/s) - ‘./cats_and_dogs_filtered.zip’ saved [68606236/68606236]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jy7comiGJS56",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "main_folder = './'\n",
        "!unzip -q cats_and_dogs_filtered.zip -d $main_folder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNEAOm1qJUVL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "030d8544-7b32-4c26-92a8-8c6e448bfc01"
      },
      "source": [
        "cd cats_and_dogs_filtered"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/cats_and_dogs_filtered\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41vjDIAgYb7b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir generated-images"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61zQgulSJV9b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_folder = './train'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9XwlX_HJjuV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_generator = ImageDataGenerator()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4KMb5EaMrhP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://stackoverflow.com/questions/6996603/how-to-delete-a-file-or-folder\n",
        "import shutil\n",
        "shutil.rmtree(\"train/cats\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abz0WDqxKPsr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4b727254-d736-43f1-8aa3-b6559770d369"
      },
      "source": [
        "generated_train_data = train_generator.flow_from_directory(\n",
        "    train_folder,\n",
        "    target_size = (128, 128),\n",
        "    batch_size = 128,\n",
        "    class_mode = 'binary'\n",
        ")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1000 images belonging to 1 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDZ3EX6eLsgT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data(pixels=128, verbose=False):\n",
        "    print(\"Loading data\")\n",
        "    X_train = []\n",
        "    paths = glob.glob(os.path.normpath(os.getcwd() + '/train/dogs/*.jpg'))\n",
        "    for path in paths:\n",
        "        if verbose: print(path)\n",
        "        im = Image.open(path)\n",
        "        im = ImageOps.fit(im, (pixels, pixels), Image.ANTIALIAS)\n",
        "        im = ImageOps.grayscale(im)\n",
        "        #im.show()\n",
        "        im = np.asarray(im)\n",
        "        X_train.append(im)\n",
        "    print(\"Finished loading data\")\n",
        "    return np.array(X_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ga5izMvNJN1c",
        "colab_type": "text"
      },
      "source": [
        "#Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "9FVmGybpOrj3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generator_model():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(input_dim=100, output_dim=1024))\n",
        "    model.add(Activation('tanh'))\n",
        "    model.add(Dense(128*8*8))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('tanh'))\n",
        "    model.add(Reshape((128, 8, 8), input_shape=(128*8*8,)))\n",
        "    model.add(UpSampling2D(size=(4, 4)))\n",
        "    model.add(Convolution2D(64, 5, 5, border_mode='same'))\n",
        "    model.add(Activation('tanh'))\n",
        "    model.add(UpSampling2D(size=(4, 4)))\n",
        "    model.add(Convolution2D(1, 5, 5, border_mode='same'))\n",
        "    model.add(Activation('tanh'))\n",
        "    return model\n",
        "\n",
        "\n",
        "def discriminator_model():\n",
        "    model = Sequential()\n",
        "    model.add(Convolution2D(\n",
        "                        64, 5, 5,\n",
        "                        border_mode='same',\n",
        "                        input_shape=(1, 128, 128)))\n",
        "    model.add(Activation('tanh'))\n",
        "    model.add(AveragePooling2D(pool_size=(4, 4)))\n",
        "    model.add(Convolution2D(128, 5, 5))\n",
        "    model.add(Activation('tanh'))\n",
        "    model.add(AveragePooling2D(pool_size=(2, 2)))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(256))\n",
        "    model.add(Activation('tanh'))\n",
        "    model.add(Dense(1))\n",
        "    model.add(Activation('sigmoid'))\n",
        "    return model\n",
        "\n",
        "\n",
        "def generator_containing_discriminator(generator, discriminator):\n",
        "    model = Sequential()\n",
        "    model.add(generator)\n",
        "    discriminator.trainable = False\n",
        "    model.add(discriminator)\n",
        "    return model\n",
        "\n",
        "\n",
        "def combine_images(generated_images):\n",
        "    num = generated_images.shape[0]\n",
        "    width = int(math.sqrt(num))\n",
        "    height = int(math.ceil(float(num)/width))\n",
        "    shape = generated_images.shape[2:]\n",
        "    image = np.zeros((height*shape[0], width*shape[1]),\n",
        "                     dtype=generated_images.dtype)\n",
        "    for index, img in enumerate(generated_images):\n",
        "        i = int(index/width)\n",
        "        j = index % width\n",
        "        image[i*shape[0]:(i+1)*shape[0], j*shape[1]:(j+1)*shape[1]] = \\\n",
        "            img[0, :, :]\n",
        "    return image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "JrUB9sIJOrj_",
        "colab_type": "code",
        "outputId": "4f0cf76a-8652-413d-a15a-1b197020fee7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        }
      },
      "source": [
        "model = generator_model()\n",
        "print(model.summary())"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(input_dim=100, units=1024)`\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 1024)              103424    \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 8192)              8396800   \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 8192)              32768     \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 8192)              0         \n",
            "_________________________________________________________________\n",
            "reshape_1 (Reshape)          (None, 128, 8, 8)         0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d_1 (UpSampling2 (None, 128, 32, 32)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 64, 32, 32)        204864    \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 64, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d_2 (UpSampling2 (None, 64, 128, 128)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 1, 128, 128)       1601      \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 1, 128, 128)       0         \n",
            "=================================================================\n",
            "Total params: 8,739,457\n",
            "Trainable params: 8,723,073\n",
            "Non-trainable params: 16,384\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (5, 5), padding=\"same\")`\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(1, (5, 5), padding=\"same\")`\n",
            "  del sys.path[0]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "a_IN0rKEOrkE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(epochs, BATCH_SIZE, weights=False):\n",
        "    \"\"\"\n",
        "    :param epochs: Train for this many epochs\n",
        "    :param BATCH_SIZE: Size of minibatch\n",
        "    :param weights: If True, load weights from file, otherwise train the model from scratch. \n",
        "    Use this if you have already saved state of the network and want to train it further.\n",
        "    \"\"\"\n",
        "    X_train = load_data()\n",
        "    X_train = (X_train.astype(np.float32) - 127.5)/127.5\n",
        "    X_train = X_train.reshape((X_train.shape[0], 1) + X_train.shape[1:])\n",
        "    discriminator = discriminator_model()\n",
        "    generator = generator_model()\n",
        "    if weights:\n",
        "        generator.load_weights('goodgenerator.h5')\n",
        "        discriminator.load_weights('gooddiscriminator.h5')\n",
        "    discriminator_on_generator = \\\n",
        "        generator_containing_discriminator(generator, discriminator)\n",
        "    d_optim = SGD(lr=0.0005, momentum=0.9, nesterov=True)\n",
        "    g_optim = SGD(lr=0.0005, momentum=0.9, nesterov=True)\n",
        "    generator.compile(loss='binary_crossentropy', optimizer=\"SGD\")\n",
        "    discriminator_on_generator.compile(\n",
        "        loss='binary_crossentropy', optimizer=g_optim)\n",
        "    discriminator.trainable = True\n",
        "    discriminator.compile(loss='binary_crossentropy', optimizer=d_optim)\n",
        "    noise = np.zeros((BATCH_SIZE, 100))\n",
        "    for epoch in range(epochs):\n",
        "        print(\"Epoch is\", epoch)\n",
        "        print(\"Number of batches\", int(X_train.shape[0]/BATCH_SIZE))\n",
        "        for index in range(int(X_train.shape[0]/BATCH_SIZE)):\n",
        "            for i in range(BATCH_SIZE):\n",
        "                noise[i, :] = np.random.uniform(-1, 1, 100)\n",
        "            image_batch = X_train[index*BATCH_SIZE:(index+1)*BATCH_SIZE]\n",
        "            generated_images = generator.predict(noise, verbose=0)\n",
        "            #print(generated_images.shape)\n",
        "            if index % 20 == 0 and epoch % 10 == 0:\n",
        "                image = combine_images(generated_images)\n",
        "                image = image*127.5+127.5\n",
        "                destpath = os.path.normpath(os.getcwd()+ \"/generated-images/\"+str(epoch)+\"_\"+str(index)+\".png\")\n",
        "                Image.fromarray(image.astype(np.uint8)).save(destpath)\n",
        "            X = np.concatenate((image_batch, generated_images))\n",
        "            y = [1] * BATCH_SIZE + [0] * BATCH_SIZE\n",
        "            d_loss = discriminator.train_on_batch(X, y)\n",
        "            print(\"batch %d d_loss : %f\" % (index, d_loss))\n",
        "            for i in range(BATCH_SIZE):\n",
        "                noise[i, :] = np.random.uniform(-1, 1, 100)\n",
        "            discriminator.trainable = False\n",
        "            g_loss = discriminator_on_generator.train_on_batch(\n",
        "                noise, [1] * BATCH_SIZE)\n",
        "            discriminator.trainable = True\n",
        "            print(\"batch %d g_loss : %f\" % (index, g_loss))\n",
        "            if epoch % 10 == 9:\n",
        "                generator.save_weights('goodgenerator.h5', True)\n",
        "                discriminator.save_weights('gooddiscriminator.h5', True)\n",
        "\n",
        "def clean(image):\n",
        "    for i in range(1, image.shape[0] - 1):\n",
        "        for j in range(1, image.shape[1] - 1):\n",
        "            if image[i][j] + image[i+1][j] + image[i][j+1] + image[i-1][j] + image[i][j-1] > 127 * 5:\n",
        "                image[i][j] = 255\n",
        "    return image\n",
        "def generate(BATCH_SIZE):\n",
        "    generator = generator_model()\n",
        "    generator.compile(loss='binary_crossentropy', optimizer=\"SGD\")\n",
        "    generator.load_weights('goodgenerator.h5')\n",
        "    noise = np.zeros((BATCH_SIZE, 100))\n",
        "    a = np.random.uniform(-1, 1, 100)\n",
        "    b = np.random.uniform(-1, 1, 100)\n",
        "    grad = (b - a) / BATCH_SIZE\n",
        "    for i in range(BATCH_SIZE):\n",
        "        noise[i, :] = np.random.uniform(-1, 1, 100)\n",
        "    generated_images = generator.predict(noise, verbose=1)\n",
        "    #image = combine_images(generated_images)\n",
        "    print(generated_images.shape)\n",
        "    for image in generated_images:\n",
        "        image = image[0]\n",
        "        image = image*127.5+127.5\n",
        "        Image.fromarray(image.astype(np.uint8)).save(\"dirty.png\")\n",
        "        Image.fromarray(image.astype(np.uint8)).show()\n",
        "        clean(image)\n",
        "        image = Image.fromarray(image.astype(np.uint8))\n",
        "        image.show()        \n",
        "        image.save(\"clean.png\")\n",
        "\n",
        "\n",
        "def get_args():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--mode\", type=str)\n",
        "    parser.add_argument(\"--batch_size\", type=int, default=128)\n",
        "    parser.add_argument(\"--nice\", dest=\"nice\", action=\"store_true\")\n",
        "    parser.set_defaults(nice=False)\n",
        "    args = parser.parse_args()\n",
        "    return args\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "0k8TmQM6OrkJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 989
        },
        "outputId": "88f7b21a-146c-4a88-be4e-fffbe047e542"
      },
      "source": [
        "train(400, 100, False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading data\n",
            "Finished loading data\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (5, 5), input_shape=(1, 128, 1..., padding=\"same\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:26: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (5, 5))`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(input_dim=100, units=1024)`\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (5, 5), padding=\"same\")`\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(1, (5, 5), padding=\"same\")`\n",
            "  del sys.path[0]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch is 0\n",
            "Number of batches 10\n",
            "batch 0 d_loss : 0.689302\n",
            "batch 0 g_loss : 0.693104\n",
            "batch 1 d_loss : 0.688419\n",
            "batch 1 g_loss : 0.694151\n",
            "batch 2 d_loss : 0.689926\n",
            "batch 2 g_loss : 0.695599\n",
            "batch 3 d_loss : 0.688508\n",
            "batch 3 g_loss : 0.696549\n",
            "batch 4 d_loss : 0.687413\n",
            "batch 4 g_loss : 0.695479\n",
            "batch 5 d_loss : 0.688206\n",
            "batch 5 g_loss : 0.697552\n",
            "batch 6 d_loss : 0.680119\n",
            "batch 6 g_loss : 0.696632\n",
            "batch 7 d_loss : 0.682030\n",
            "batch 7 g_loss : 0.693704\n",
            "batch 8 d_loss : 0.682610\n",
            "batch 8 g_loss : 0.693083\n",
            "batch 9 d_loss : 0.681283\n",
            "batch 9 g_loss : 0.691954\n",
            "Epoch is 1\n",
            "Number of batches 10\n",
            "batch 0 d_loss : 0.679116\n",
            "batch 0 g_loss : 0.695167\n",
            "batch 1 d_loss : 0.679131\n",
            "batch 1 g_loss : 0.697293\n",
            "batch 2 d_loss : 0.683036\n",
            "batch 2 g_loss : 0.692885\n",
            "batch 3 d_loss : 0.677410\n",
            "batch 3 g_loss : 0.693810\n",
            "batch 4 d_loss : 0.679381\n",
            "batch 4 g_loss : 0.694822\n",
            "batch 5 d_loss : 0.684161\n",
            "batch 5 g_loss : 0.691994\n",
            "batch 6 d_loss : 0.673126\n",
            "batch 6 g_loss : 0.691493\n",
            "batch 7 d_loss : 0.677495\n",
            "batch 7 g_loss : 0.688560\n",
            "batch 8 d_loss : 0.677468\n",
            "batch 8 g_loss : 0.687002\n",
            "batch 9 d_loss : 0.677615\n",
            "batch 9 g_loss : 0.682488\n",
            "Epoch is 2\n",
            "Number of batches 10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "scrolled": false,
        "id": "hhfsXwPAOrkP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generate(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "viLdOrdEOrkV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}